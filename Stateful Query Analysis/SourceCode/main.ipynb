{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1GmJyh0L5g6"
      },
      "outputs": [],
      "source": [
        "#import the required libraries\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from glob import glob \n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import GradientTape\n",
        "from tensorflow.train import Checkpoint, CheckpointManager\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.layers import Dense, Convolution2D, MaxPool2D, Flatten, BatchNormalization, Dropout, Input\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "import datetime\n",
        "import sklearn.metrics.pairwise as pairwise\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "from itertools import combinations\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9Y2foOj6SbC"
      },
      "outputs": [],
      "source": [
        "from utilities import dir_transformation, encoder_dir_transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN2EzUC0SVTo"
      },
      "outputs": [],
      "source": [
        "#The input size for the malware classifier and similarity encoder\n",
        "w1, h2 = 224, 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix5DEUgkoM5e"
      },
      "outputs": [],
      "source": [
        "#The path to the malware samples\n",
        "malware_dir = 'Dataset/Malware'\n",
        "malware_add = glob(malware_dir + '/*')\n",
        "malware_add = sorted(malware_add)\n",
        "\n",
        "#The path to the benign samples\n",
        "benign_dir = 'Dataset/Benign'\n",
        "benign_add = glob(benign_dir + '/*')\n",
        "benign_add = sorted(benign_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "6nzKkBaVK63d",
        "outputId": "4ea62db7-1de4-472d-92ae-7557f18b1768"
      },
      "outputs": [],
      "source": [
        "# use dir_transformation from utilities.py to resize images to size w x h.\n",
        "malware_images = dir_transformation(malware_add, w1, h2)\n",
        "benign_images = dir_transformation(benign_add, w1, h2)\n",
        "\n",
        "data = np.array(benign_images + malware_images)\n",
        "\n",
        "# Make one-hot labels for samples\n",
        "labels = np.array([[1, 0] for _ in range(len(benign_images))] + [[0, 1] for _ in range(len(malware_images))])\n",
        "\n",
        "#Shuffle the dataset and split to train and test sets (20% test set)\n",
        "idx = np.random.RandomState(seed=42).permutation(data.shape[0])\n",
        "data, labels = data[idx], labels[idx]\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).shuffle(1102).batch(150)\n",
        "test_dataset =  tf.data.Dataset.from_tensor_slices((test_data, test_labels)).batch(256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGT2HWlBmcn5"
      },
      "outputs": [],
      "source": [
        "#The function that returns an instance of the malware classifier\n",
        "def classifier():\n",
        "  model = Sequential()\n",
        "  model.add(Convolution2D(filters = 8, kernel_size = 3, padding = 'same', input_shape = [h2, w1, 1], activation = 'relu', name = 'conv_1'))\n",
        "  model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same', name = 'maxpool_1'))\n",
        "\n",
        "  model.add(Convolution2D(filters = 4, kernel_size = 3, padding = 'same', activation = 'relu', name = 'conv_2'))\n",
        "  model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same', name = 'maxpool_2'))\n",
        "\n",
        "  model.add(Convolution2D(filters = 2, kernel_size = 3, padding = 'same', activation = 'relu', name = 'conv_3'))\n",
        "  model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same', name = 'maxpool_3'))\n",
        "\n",
        "  model.add(Convolution2D(filters = 2, kernel_size = 3, padding = 'same', activation = 'relu', name = 'conv_4'))\n",
        "  model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same', name = 'maxpool_4'))\n",
        "\n",
        "  model.add(Convolution2D(filters = 2, kernel_size = 3, padding = 'same', activation = 'relu', name = 'conv_5'))\n",
        "  model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same', name = 'maxpool_5'))\n",
        "\n",
        "  model.add(Flatten(name = 'flatten'))\n",
        "\n",
        "  model.add(Dense(2, activation = 'softmax', name = 'softmax'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EztU6yPzayIq"
      },
      "outputs": [],
      "source": [
        "# create an instance of the classifier and set the loss and optimizer.\n",
        "model = classifier()\n",
        "optimizer = Adam()\n",
        "loss = CategoricalCrossentropy()\n",
        "model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_weights('models/classifier_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv_2DE1fa0ow",
        "outputId": "91cd1474-8e6e-4245-f5a0-d7f23d2dcc03"
      },
      "outputs": [],
      "source": [
        "#print a summary of the architecture of the model.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ10TI83TJrL",
        "outputId": "ea059ecd-bf3a-4a2e-940f-2719a12987a3"
      },
      "outputs": [],
      "source": [
        "model.fit(train_dataset, validation_data = test_dataset, epochs = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_QA9wQkbKPv"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('models'):\n",
        "    os.mkdir('models')\n",
        "    \n",
        "model.save_weights('models/classifier_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOEB-nHu3Q5e"
      },
      "outputs": [],
      "source": [
        "# Generate a copy of the classifier, set its optimizer and loss funcitons and load the weight of the initial classifier into it.\n",
        "model_copy = tf.keras.models.clone_model(model)\n",
        "optimizer = Adam()\n",
        "loss = CategoricalCrossentropy()\n",
        "model_copy.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
        "model_copy.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wADHahELR-P"
      },
      "outputs": [],
      "source": [
        "#Prepare the dataset for training the encoder. The difference here is the fact that we only set value of w for every sample and each sample\n",
        "#could have a different value for h.\n",
        "malware_images = encoder_dir_transformation(malware_add, w1)\n",
        "benign_images = encoder_dir_transformation(benign_add, w1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62U0QM0UG_bp",
        "outputId": "dfbb4e78-267e-4e47-de91-22591f65e6c2"
      },
      "outputs": [],
      "source": [
        "#Prepare x_i. \n",
        "data_i = np.array(benign_images + malware_images)\n",
        "data_l = np.array([0 for _ in range(len(benign_images))] + [1 for _ in range(len(malware_images))])\n",
        "i_height = np.array([i.shape[0] for i in data_i])\n",
        "\n",
        "idx = np.random.RandomState(seed=42).permutation(data_i.shape[0])\n",
        "data_i, data_l, i_height = data_i[idx], data_l[idx], i_height[idx]\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data_i, data_l, test_size=0.2, random_state=42)\n",
        "\n",
        "data_dict = {'train': train_data, 'test': test_data}\n",
        "label_dict = {'train': train_labels, 'test': test_labels}\n",
        "height_dict = {'train': np.array([i.shape[0] for i in train_data]), 'test': np.array([i.shape[0] for i in test_data])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zy5J3a-mJcP"
      },
      "outputs": [],
      "source": [
        "#The function that performs the swapping transformation.\n",
        "def swap_transform(train_or_test, i, i_idx, portion_swap_pct):\n",
        "  #What portion of i for swaping\n",
        "  portion_length = np.ceil(i.shape[0] * portion_swap_pct)\n",
        "  #Find data with the length higher or equle to the swaping portion in the same class and choose one randomly\n",
        "  data_l_copy = np.copy(label_dict[train_or_test])\n",
        "  data_l_copy[i_idx] = 1 - label_dict[train_or_test][i_idx]\n",
        "  p_idx = np.where(data_l_copy == label_dict[train_or_test][i_idx])[0]\n",
        "  p_idx = np.intersect1d(p_idx, np.where(height_dict[train_or_test] >= portion_length)[0])\n",
        "\n",
        "  if p_idx.shape[0] == 0:\n",
        "    print(f'0 data with shape greater than {portion_swap_pct} of i')\n",
        "  p_idx = np.random.choice(p_idx, 1)[0]\n",
        "\n",
        "  swap_sample = data_dict[train_or_test][p_idx]\n",
        "  swap_start_pointer = np.random.choice(np.arange(swap_sample.shape[0] - portion_length + 1), 1)\n",
        "  i_start_pointer = np.random.choice(np.arange(i.shape[0] - portion_length + 1), 1)\n",
        "  p = np.copy(i)\n",
        "  p[int(i_start_pointer):int(i_start_pointer + portion_length),...] = swap_sample[int(swap_start_pointer):int(swap_start_pointer + portion_length),...]\n",
        "\n",
        "  return p\n",
        "\n",
        "#The function that performs the noise replacement option.\n",
        "def noisy_transform(i, noise_pct):\n",
        "  random_indices = np.random.choice(np.arange(i.size), replace=False, size=int(i.size * noise_pct))\n",
        "  raveld = np.copy(i).ravel()\n",
        "  raveld[random_indices] = np.random.uniform(low = 0., high = 1., size = random_indices.size)\n",
        "  raveld = raveld.reshape(i.shape)\n",
        "  return raveld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxe9LdmYsOm1"
      },
      "outputs": [],
      "source": [
        "#The function to generate x_i, x_p, x_j and x_n in each transformation.\n",
        "\n",
        "def batch_transform(train_or_test, data_batch, batch_idx):\n",
        "  batch_i, batch_p, batch_j, batch_n = [], [], [], []\n",
        "  for i, i_idx in zip(data_batch, batch_idx):\n",
        "\n",
        "    # -------------- swap only ---------------\n",
        "    if portion_swap_pct != 0.0:\n",
        "      p = swap_transform(train_or_test, i, i_idx, portion_swap_pct)\n",
        "    \n",
        "    # -------------- noise only ---------------\n",
        "    elif noise_pct != 0.0:\n",
        "      p = noisy_transform(i, noise_pct)\n",
        "\n",
        "\n",
        "    j_idx = np.random.choice(np.arange(data_dict[train_or_test].shape[0]), 1)\n",
        "    n_idx = np.random.choice(np.where(label_dict[train_or_test] != label_dict[train_or_test][j_idx])[0], 1)\n",
        "\n",
        "    j, n = data_dict[train_or_test][j_idx][0], data_dict[train_or_test][n_idx][0]\n",
        "\n",
        "    batch_i.append(np.expand_dims(cv2.resize(i, (w1, h2), interpolation = cv2.INTER_AREA), axis = -1))\n",
        "    batch_p.append(np.expand_dims(cv2.resize(p, (w1, h2), interpolation = cv2.INTER_AREA), axis = -1))\n",
        "    batch_j.append(np.expand_dims(cv2.resize(j, (w1, h2), interpolation = cv2.INTER_AREA), axis = -1))\n",
        "    batch_n.append(np.expand_dims(cv2.resize(n, (w1, h2), interpolation = cv2.INTER_AREA), axis = -1))\n",
        "\n",
        "  return np.array(batch_i), np.array(batch_p), np.array(batch_j), np.array(batch_n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.evaluate(test_dataset))\n",
        "print(model_copy.evaluate(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nR8CNHZeLHr"
      },
      "outputs": [],
      "source": [
        "#This dictionary is used for training and saving the results based on your chase of distance metric.\n",
        "distance_type_dict = {'euclidean': 'Euclidean', 'mahalanobis': 'Mahalanobis'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>In the below cell there are the main hyperparameters of our model (<font color='red'>distance_type, portion_swap_pct, noise_pct, EPOCHS, BATCH_SIZE and m_squared</font>). Keep in mind that we have trained the encoder with the assumption that only one of the two preprocess (portion_swap_pct and noise_pct) options can be non-zero (from [0.05, 0.1, 0.15, 0.2]) and the other should be zero.</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsMZppyVuFZC",
        "outputId": "6cfb186a-1021-46aa-d103-52c5f1169dac"
      },
      "outputs": [],
      "source": [
        "#Here please choose your required distance metrics: euclidean or mahalanobis.\n",
        "# distance_type = distance_type_dict['euclidean']\n",
        "distance_type = distance_type_dict['mahalanobis']\n",
        "\n",
        "#The ratio of preprocess. One should be equal zero.\n",
        "portion_swap_pct = 0.0\n",
        "noise_pct = 0.1\n",
        "\n",
        "#The number of epochs and the batch size for training the encoder.\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "#choose the value of m_squared.\n",
        "m_squared = 100\n",
        "m = np.sqrt(m_squared).astype(np.float32)\n",
        "\n",
        "#The directory to save the results for the encoder.\n",
        "encoder_save_add = f'models/encoder_weights_{distance_type}_{int(100 * noise_pct)}pctNoise_{int(100 * portion_swap_pct)}pctSwap_msqr{m_squared}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "7rnDT0LUPjeS",
        "outputId": "d7013db3-a8ab-4cb5-cae9-f9f10e237f2a"
      },
      "outputs": [],
      "source": [
        "# Set the embedding dimensionality and learning rate.\n",
        "embedding_dim = 128\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Set the optimizer, freeze the parameters for every layer except the embedding layer and then create the encoder model.\n",
        "encoder_opt = Adam(learning_rate = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc7rNX-01aEh"
      },
      "outputs": [],
      "source": [
        "#set up the checkpoint manager to save the encoder later\n",
        "checkpoint = Checkpoint(encoder_optimizer = encoder_opt, encoder = encoder)\n",
        "manager = CheckpointManager(checkpoint=checkpoint, directory=encoder_save_add, max_to_keep=1)\n",
        "\n",
        "#set up the summary wirters for training and testing to store the loss value in each batch for each epoch.\n",
        "encoder_plot_add = f'plots/{distance_type}_{int(100 * noise_pct)}pctNoise_{int(100 * portion_swap_pct)}pctSwap_msqr{m_squared}'\n",
        "encoder_train_summary_writer = tf.summary.create_file_writer(encoder_plot_add + \"/logs/train_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "encoder_test_summary_writer = tf.summary.create_file_writer(encoder_plot_add + \"/logs/test_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xcZia6AP0XZ"
      },
      "outputs": [],
      "source": [
        "# The funciton used during the training process. This funciton is called in each batch for each epoch. \n",
        "# Depending on the selected distance metric, the loss function is measured differently. The loss function is recorded using the summary writer.\n",
        "# Using the gradient of the loss function with respect to the parameters of the encoder, we can optimize the same parameters.\n",
        "\n",
        "@tf.function\n",
        "def train_step(xi, xp, xj, xn, epoch):\n",
        "  # ------------------- Euclidean Distance -------------------\n",
        "  if distance_type == 'Euclidean':\n",
        "    with GradientTape() as tape:\n",
        "      ei = encoder(xi)\n",
        "      ep = encoder(xp) \n",
        "      ej = encoder(xj)\n",
        "      en = encoder(xn)\n",
        "\n",
        "      loss_l2_ip = tf.reduce_mean(tf.square(ei - ep))\n",
        "      loss_l2_jn = tf.reduce_mean(tf.maximum(0.0, tf.math.pow(m, 2) - tf.reduce_mean(tf.square(ej - en), axis = -1)))\n",
        "      loss_l2_total = loss_l2_ip + loss_l2_jn\n",
        "      \n",
        "    gradients = tape.gradient(loss_l2_total, encoder.trainable_variables) \n",
        "    encoder_opt.apply_gradients(zip(gradients, encoder.trainable_variables))\n",
        "\n",
        "    with encoder_train_summary_writer.as_default():\n",
        "      tf.summary.scalar('l2_loss_ip', loss_l2_ip, step=tf.cast(epoch, 'int64'))\n",
        "      tf.summary.scalar('l2_loss_jn', loss_l2_jn, step=tf.cast(epoch, 'int64'))\n",
        "      tf.summary.scalar('l2_loss_total', loss_l2_total, step=tf.cast(epoch, 'int64'))\n",
        "\n",
        "  # ------------------- Mahalanobis Distance -------------------\n",
        "  elif distance_type == 'Mahalanobis':\n",
        "    with GradientTape() as tape:\n",
        "      ei = encoder(xi)\n",
        "      ep = encoder(xp) \n",
        "      ej = encoder(xj)\n",
        "      en = encoder(xn)\n",
        "\n",
        "      ei = tf.expand_dims(ei, axis = 1)\n",
        "      ep = tf.expand_dims(ep, axis = 1)\n",
        "      ej = tf.expand_dims(ej, axis = 1)\n",
        "      en = tf.expand_dims(en, axis = 1)\n",
        "\n",
        "\n",
        "      cov_matrix_ip = tf.reduce_sum(tf.matmul(tf.math.conj(tf.transpose(ei - tf.reduce_mean(ei, axis = 0), perm = [0, 2, 1])), ep - tf.reduce_mean(ep, axis = 0)), axis = 0) * (1 / (ei.shape[0] - 1))\n",
        "      cov_matrix_ip = tf.linalg.inv(cov_matrix_ip)\n",
        "      loss_mahalanobis_ip = tf.reduce_mean(tf.squeeze(tf.matmul(tf.matmul(ei - ep, cov_matrix_ip), tf.math.conj(tf.transpose(ei - ep, perm=[0, 2, 1])))))\n",
        "      \n",
        "      cov_matrix_jn = tf.reduce_sum(tf.matmul(tf.math.conj(tf.transpose(ej - tf.reduce_mean(ej, axis = 0), perm = [0, 2, 1])), en - tf.reduce_mean(en, axis = 0)), axis = 0) * (1 / (ej.shape[0] - 1))\n",
        "      cov_matrix_jn = tf.linalg.inv(cov_matrix_jn)\n",
        "      loss_mahalanobis_jn = tf.matmul(tf.matmul(ej - en, cov_matrix_jn), tf.math.conj(tf.transpose(ej - en, perm=[0, 2, 1])))\n",
        "      loss_mahalanobis_jn = tf.reduce_mean(tf.maximum(0.0, tf.math.pow(m, 2) - tf.squeeze(loss_mahalanobis_jn)))\n",
        "\n",
        "      loss_mahalanobis_total = loss_mahalanobis_ip + loss_mahalanobis_jn\n",
        "\n",
        "    gradients = tape.gradient(loss_mahalanobis_total, encoder.trainable_variables)\n",
        "    encoder_opt.apply_gradients(zip(gradients, encoder.trainable_variables))\n",
        "\n",
        "    with encoder_train_summary_writer.as_default():\n",
        "      tf.summary.scalar('mahalanobis_loss_ip', loss_mahalanobis_ip, step=tf.cast(epoch, 'int64'))\n",
        "      tf.summary.scalar('mahalanobis_loss_jn', loss_mahalanobis_jn, step=tf.cast(epoch, 'int64'))\n",
        "      tf.summary.scalar('mahalanobis_loss_total', loss_mahalanobis_total, step=tf.cast(epoch, 'int64'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdN0ZirA14Qf"
      },
      "outputs": [],
      "source": [
        "# The funciton used during the testing process. This funciton is called in each batch for each epoch. \n",
        "# Depending on the selected distance metric, the loss function is measured differently. The loss function is recorded using the summary writer.\n",
        "\n",
        "@tf.function\n",
        "def test_step(xi, xp, xj, xn, epoch):\n",
        "  # ------------ Euclidean Distance ------------\n",
        "  if distance_type == 'Euclidean':  \n",
        "    with GradientTape() as tape:\n",
        "      ei = encoder(xi)\n",
        "      ep = encoder(xp) \n",
        "      ej = encoder(xj)\n",
        "      en = encoder(xn)\n",
        "\n",
        "      loss_l2_ip = tf.reduce_mean(tf.square(ei - ep))\n",
        "      loss_l2_jn = tf.reduce_mean(tf.maximum(0.0, tf.math.pow(m, 2) - tf.reduce_mean(tf.square(ej - en), axis = -1)))\n",
        "      loss_l2_total = loss_l2_ip + loss_l2_jn\n",
        "\n",
        "    with encoder_test_summary_writer.as_default():\n",
        "      tf.summary.scalar('l2_loss_ip', loss_l2_ip, step=tf.cast(epoch, 'int64'))\n",
        "      tf.summary.scalar('l2_loss_jn', loss_l2_jn, step=tf.cast(epoch, 'int64'))\n",
        "      tf.summary.scalar('l2_loss_total', loss_l2_total, step=tf.cast(epoch, 'int64'))\n",
        "\n",
        "  # ------------------- Mahalanobis Distance -------------------\n",
        "  elif distance_type == 'Mahalanobis':  \n",
        "\n",
        "    with GradientTape() as tape:\n",
        "      ei = encoder(xi)\n",
        "      ep = encoder(xp) \n",
        "      ej = encoder(xj)\n",
        "      en = encoder(xn)\n",
        "\n",
        "      ei = tf.expand_dims(ei, axis = 1)\n",
        "      ep = tf.expand_dims(ep, axis = 1)\n",
        "      ej = tf.expand_dims(ej, axis = 1)\n",
        "      en = tf.expand_dims(en, axis = 1)\n",
        "\n",
        "      cov_matrix_ip = tf.reduce_sum(tf.matmul(tf.math.conj(tf.transpose(ei - tf.reduce_mean(ei, axis = 0), perm = [0, 2, 1])), ep - tf.reduce_mean(ep, axis = 0)), axis = 0) * (1 / (ei.shape[0] - 1))\n",
        "      cov_matrix_ip = tf.linalg.inv(cov_matrix_ip)\n",
        "      loss_mahalanobis_ip = tf.reduce_mean(tf.squeeze(tf.matmul(tf.matmul(ei - ep, cov_matrix_ip), tf.math.conj(tf.transpose(ei - ep, perm=[0, 2, 1])))))\n",
        "\n",
        "      cov_matrix_jn = tf.reduce_sum(tf.matmul(tf.math.conj(tf.transpose(ej - tf.reduce_mean(ej, axis = 0), perm = [0, 2, 1])), en - tf.reduce_mean(en, axis = 0)), axis = 0) * (1 / (ej.shape[0] - 1))\n",
        "      cov_matrix_jn = tf.linalg.inv(cov_matrix_jn)\n",
        "      loss_mahalanobis_jn = tf.matmul(tf.matmul(ej - en, cov_matrix_jn), tf.math.conj(tf.transpose(ej - en, perm=[0, 2, 1])))\n",
        "      loss_mahalanobis_jn = tf.reduce_mean(tf.maximum(0.0, tf.math.pow(m, 2) - tf.squeeze(loss_mahalanobis_jn)))\n",
        "\n",
        "      loss_mahalanobis_total = loss_mahalanobis_ip + loss_mahalanobis_jn\n",
        "\n",
        "    with encoder_test_summary_writer.as_default():\n",
        "      tf.summary.scalar('mahalanobis_loss_ip', loss_mahalanobis_ip, step=tf.cast(epoch, 'int64'))\n",
        "      tf.summary.scalar('mahalanobis_loss_jn', loss_mahalanobis_jn, step=tf.cast(epoch, 'int64'))\n",
        "      tf.summary.scalar('mahalanobis_loss_total', loss_mahalanobis_total, step=tf.cast(epoch, 'int64'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZoVk5GvrfII"
      },
      "outputs": [],
      "source": [
        "# This function recieves a series of batches of (x_i, x_p, x_j, x_n)s and then call the train_step and test_step functions. This process is repeated for a set of epochs which is defined\n",
        "# as argument epochs. At the end of each epoch, the dataset is shuffled.\n",
        "\n",
        "def train_test(epochs):\n",
        "  for epoch in tf.range(epochs):\n",
        "    print(f'Epoch {epoch}')\n",
        "    for batch in range(int(np.ceil(data_dict['train'].shape[0] / BATCH_SIZE))):\n",
        "      batch_i, batch_p, batch_j, batch_n = None, None, None, None\n",
        "\n",
        "      if (batch + 1) * BATCH_SIZE > data_dict['train'].shape[0] - 1:\n",
        "        data_batch = data_dict['train'][batch * BATCH_SIZE:data_dict['train'].shape[0]]\n",
        "\n",
        "\n",
        "        batch_i, batch_p, batch_j, batch_n = batch_transform('train', data_batch, range(batch * BATCH_SIZE, data_dict['train'].shape[0]))\n",
        "      else:\n",
        "        data_batch = data_dict['train'][batch * BATCH_SIZE:(batch + 1) * BATCH_SIZE]\n",
        "        batch_i, batch_p, batch_j, batch_n = batch_transform('train', data_batch, range(batch * BATCH_SIZE, (batch + 1) * BATCH_SIZE))\n",
        "      train_step(batch_i, batch_p, batch_j, batch_n, epoch)\n",
        "\n",
        "\n",
        "    for batch in range(int(np.ceil(data_dict['test'].shape[0] / BATCH_SIZE))):\n",
        "      batch_i, batch_p, batch_j, batch_n = None, None, None, None\n",
        "\n",
        "      if (batch + 1) * BATCH_SIZE > data_dict['test'].shape[0] - 1:\n",
        "        data_batch = data_dict['test'][batch * BATCH_SIZE:data_dict['test'].shape[0]]\n",
        "\n",
        "        batch_i, batch_p, batch_j, batch_n = batch_transform('test', data_batch, range(batch * BATCH_SIZE, data_dict['test'].shape[0]))\n",
        "      else:\n",
        "        data_batch = data_dict['test'][batch * BATCH_SIZE:(batch + 1) * BATCH_SIZE]\n",
        "        batch_i, batch_p, batch_j, batch_n = batch_transform('test', data_batch, range(batch * BATCH_SIZE, (batch + 1) * BATCH_SIZE))\n",
        "      test_step(batch_i, batch_p, batch_j, batch_n, epoch)\n",
        "\n",
        "    idx = np.random.RandomState().permutation(data_dict['train'].shape[0])\n",
        "    data_dict['train'], label_dict['train'], height_dict['train'] = data_dict['train'][idx], label_dict['train'][idx], height_dict['train'][idx]\n",
        "\n",
        "    idx = np.random.RandomState().permutation(data_dict['test'].shape[0])\n",
        "    data_dict['test'], label_dict['test'], height_dict['test'] = data_dict['test'][idx], label_dict['test'][idx], height_dict['test'][idx]\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "ioEa5YgyZLO5",
        "outputId": "e5e53866-fa5a-4078-b5bc-72d6132f5444"
      },
      "outputs": [],
      "source": [
        "train_test(EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0AgTi2rgbTuK",
        "outputId": "99924001-5b22-4c98-a574-740740024a1b"
      },
      "outputs": [],
      "source": [
        "# Save the encoder model at the directory that was selected before.\n",
        "manager.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi6skTObpe12"
      },
      "outputs": [],
      "source": [
        "# Show the training and testing loss functions with Tensorboard.\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir encoder_plot_add"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAMeSMohcJAL"
      },
      "outputs": [],
      "source": [
        "train_data_resized = np.array([np.expand_dims(cv2.resize(d, (w1, h2), interpolation = cv2.INTER_AREA), axis = -1) for d in train_data])\n",
        "test_data_resized = np.array([np.expand_dims(cv2.resize(d, (w1, h2), interpolation = cv2.INTER_AREA), axis = -1) for d in test_data])\n",
        "\n",
        "train_dataset_noLabel = tf.data.Dataset.from_tensor_slices(train_data_resized).batch(150)\n",
        "test_dataset_noLabel = tf.data.Dataset.from_tensor_slices(test_data_resized).batch(250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "-MtT6WZhwYYz",
        "outputId": "e1b47bea-41f5-4b1b-fef3-d5e4bd2b367c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Adapted from the github (https://github.com/schoyc/blackbox-detection) of Chen, Steven, Nicholas Carlini, and David Wagner. \\\n",
        "# \"Stateful detection of black-box adversarial attacks.\" Proceedings of the 1st ACM Workshop on Security and Privacy on Artificial Intelligence. 2020. \n",
        "# Function calculate_thresholds() returns the a pair of arrays. One for the K values and one for the thresholds. The threshold is dynamcily chosen for each K, such that\n",
        "# 0.1% of the training data would be detected as adversarial.\n",
        "\n",
        "def calculate_thresholds(training_data, K, encoder=lambda x: x, P=1000, up_to_K=False):\n",
        "  data = np.concatenate([encoder(batch) for batch in training_data])\n",
        "  \n",
        "  distances = []\n",
        "  for i in range(data.shape[0] // P):\n",
        "    distance_mat = pairwise.pairwise_distances(data[i * P:(i+1) * P,:], Y=data)\n",
        "    distance_mat = np.sort(distance_mat, axis=-1)\n",
        "    distance_mat_K = distance_mat[:,:K]\n",
        "    \n",
        "    distances.append(distance_mat_K)\n",
        "  distance_matrix = np.concatenate(distances, axis=0)\n",
        "  \n",
        "  start = 0 if up_to_K else K\n",
        "\n",
        "  THRESHOLDS = []\n",
        "  K_S = []\n",
        "  for k in range(start, K + 1):\n",
        "    dist_to_k_neighbors = distance_matrix[:,:k+1]\n",
        "    avg_dist_to_k_neighbors = dist_to_k_neighbors.mean(axis=-1)\n",
        "    \n",
        "    threshold = np.percentile(avg_dist_to_k_neighbors, 0.1)\n",
        "    \n",
        "    K_S.append(k)\n",
        "    THRESHOLDS.append(threshold)\n",
        "\n",
        "  return K_S, THRESHOLDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "K = 200\n",
        "Ks, Thrs = calculate_thresholds(train_dataset_noLabel, K, encoder, up_to_K=True)\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(Ks, Thrs)\n",
        "plt.xticks(range(0, K, 5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8-w0KPMyQFQ"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class Detector(object):\n",
        "\n",
        "  def __init__(self, K, threshold=None, training_data=None, chunk_size=1000, weights_path=encoder_save_add):\n",
        "    self.K = K\n",
        "    self.threshold = threshold\n",
        "    self.training_data = training_data\n",
        "\n",
        "    if self.threshold is None and self.training_data is None:\n",
        "      raise ValueError(\"Must provide explicit detection threshold or training data to calculate threshold!\")\n",
        "\n",
        "    self._init_encoder(weights_path)\n",
        "\n",
        "    if self.training_data is not None:\n",
        "      _, self.thresholds = calculate_thresholds(self.training_data, self.K, self.encode, up_to_K=False)\n",
        "      self.threshold = self.thresholds[-1]\n",
        "\n",
        "    self.num_queries = 0\n",
        "    self.buffer = []\n",
        "    self.memory = []\n",
        "    self.chunk_size = chunk_size\n",
        "\n",
        "    self.history = [] # Tracks number of queries (t) when attack was detected\n",
        "    self.history_by_attack = []\n",
        "    self.detected_dists = [] # Tracks knn-dist that was detected\n",
        "\n",
        "  def _init_encoder(self, weights_path):\n",
        "    self.encode = lambda x : encoder.predict(x)\n",
        "    self.encoder = encoder\n",
        "\n",
        "  def process(self, queries):\n",
        "    queries = self.encode(queries)\n",
        "    for query in queries:\n",
        "      self.process_query(query)\n",
        "\n",
        "  def process_query(self, query):\n",
        "    if len(self.memory) == 0 and len(self.buffer) < self.K:\n",
        "      self.buffer.append(query)\n",
        "      self.num_queries += 1\n",
        "      return False\n",
        "\n",
        "    k = self.K\n",
        "    all_dists = []\n",
        "\n",
        "    if len(self.buffer) > 0:\n",
        "      queries = np.stack(self.buffer, axis=0)\n",
        "      dists = np.linalg.norm(queries - query, axis=-1)\n",
        "      all_dists.append(dists)\n",
        "\n",
        "    for queries in self.memory:\n",
        "      dists = np.linalg.norm(queries - query, axis=-1)\n",
        "      all_dists.append(dists)\n",
        "\n",
        "    dists = np.concatenate(all_dists)\n",
        "    k_nearest_dists = np.partition(dists, k - 1)[:k, None]\n",
        "    k_avg_dist = np.mean(k_nearest_dists)\n",
        "\n",
        "    self.buffer.append(query)\n",
        "    self.num_queries += 1\n",
        "\n",
        "    if len(self.buffer) >= self.chunk_size:\n",
        "      self.memory.append(np.stack(self.buffer, axis=0))\n",
        "      self.buffer = []\n",
        "\n",
        "    is_attack = k_avg_dist < self.threshold\n",
        "    if is_attack:\n",
        "      self.history.append(self.num_queries)\n",
        "      self.detected_dists.append(k_avg_dist)\n",
        "\n",
        "\n",
        "  def clear_memory(self):\n",
        "    self.buffer = []\n",
        "    self.memory = []\n",
        "\n",
        "  def get_detections(self):\n",
        "    history = self.history\n",
        "    epochs = []\n",
        "    for i in range(len(history) - 1):\n",
        "      epochs.append(history[i + 1] - history[i])\n",
        "\n",
        "    return epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJlhAHCPwSgH",
        "outputId": "60837794-137f-45b1-f060-4aa23ddfdcca"
      },
      "outputs": [],
      "source": [
        "print(model.evaluate(test_dataset))\n",
        "print(model_copy.evaluate(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmE2A-Uvjufm",
        "outputId": "847d05dc-45a6-4323-b326-8384bd92f877"
      },
      "outputs": [],
      "source": [
        "#select train data, which have been correctly classified by the malware classifier.\n",
        "train_model_pred = np.argmax(model.predict(train_dataset_noLabel), axis = -1)\n",
        "right_train_data = np.delete(train_data_resized, np.where(train_model_pred != train_labels)[0], axis = 0)\n",
        "right_train_labels = np.delete(train_labels, np.where(train_model_pred != train_labels)[0], axis = 0)\n",
        "print('indices of removed train samples', np.where(train_model_pred != train_labels)[0])\n",
        "model.evaluate(right_train_data, to_categorical(right_train_labels))\n",
        "\n",
        "#select test data, which have been correctly classified by the malware classifier.\n",
        "test_model_pred = np.argmax(model.predict(test_dataset_noLabel), axis = -1)\n",
        "right_test_data = np.delete(test_data_resized, np.where(test_model_pred != test_labels)[0], axis = 0)\n",
        "right_test_labels = np.delete(test_labels, np.where(test_model_pred != test_labels)[0], axis = 0)\n",
        "print('indices of removed test samples', np.where(test_model_pred != test_labels)[0])\n",
        "model.evaluate(right_test_data, to_categorical(right_test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6tGVe56qZMZ",
        "outputId": "889644e3-75ed-49c9-f5a0-aa8fb0e6ff03"
      },
      "outputs": [],
      "source": [
        "# We use Foolbox to implement FGSM attacks. (https://github.com/bethgelab/foolbox)\n",
        "import foolbox as fb\n",
        "\n",
        "def generate_adv_samples(model, data, labels, num_sample, eps_range):\n",
        "  #------------- FGSM ----------------\n",
        "  random_idx = np.random.choice(np.arange(data.shape[0]), 1)[0]\n",
        "\n",
        "  raw, clipped, is_adv = None, None, None\n",
        "  fb_data = tf.constant(data[random_idx:random_idx + 1], dtype = tf.float32)\n",
        "  fb_label = tf.constant(labels[random_idx:random_idx + 1])\n",
        "\n",
        "  preprocessing = dict()\n",
        "  bounds = (0, 1)\n",
        "  fmodel = fb.TensorFlowModel(model, bounds=bounds, preprocessing=preprocessing)\n",
        "  raw, clipped, is_adv = fb.attacks.FGSM()(fmodel, fb_data, fb_label, epsilons = np.arange(eps_range[0], eps_range[1], (eps_range[1] - eps_range[0]) / num_sample))\n",
        "\n",
        "\n",
        "  return [c[0,...].numpy() for c in clipped], np.ones((num_sample)) * labels[random_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ID1GI_aZavC5"
      },
      "outputs": [],
      "source": [
        "#We measure True Positive, False Positive, True Negative, False Negative and accuracy using this function.\n",
        "def perf_measure(y_actual, y_hat):\n",
        "  TP, FP, TN, FN = 0, 0, 0, 0\n",
        "\n",
        "  for i in range(len(y_hat)): \n",
        "    if y_actual[i]==y_hat[i]==1:\n",
        "      TP += 1\n",
        "    elif y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
        "      FP += 1\n",
        "    elif y_actual[i]==y_hat[i]==0:\n",
        "      TN += 1\n",
        "    elif y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
        "      FN += 1\n",
        "\n",
        "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "  return (TP, FP, TN, FN), accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqqL3-rIDH5Q"
      },
      "outputs": [],
      "source": [
        "# Run adversarial attack for a specific number of rounds and return the average detection rate for the encoder \n",
        "# and the average misclassification rate caused by the attack on the malware classifier.\n",
        "\n",
        "def report_attack_results(k, round_num, num_samples):  \n",
        "  round_detections = []\n",
        "  missclass = []\n",
        "\n",
        "  for r_idx in range(round_num):\n",
        "    detector = Detector(K=k, training_data=train_dataset_noLabel)\n",
        "\n",
        "    benign_queries, benign_labels = train_data_resized, train_labels\n",
        "\n",
        "    suspicious_queries, suspicious_labels = generate_adv_samples(model, right_train_data, right_train_labels, num_samples, [0.01, 0.3])\n",
        "\n",
        "    suspicious_queries = np.array(suspicious_queries)\n",
        "\n",
        "    detector.process(benign_queries)\n",
        "    detections = detector.get_detections()\n",
        "\n",
        "    detector.process(suspicious_queries)\n",
        "    detections = detector.get_detections()\n",
        "    np_hist = np.array(detector.history)\n",
        "    suspicious_detection = np.where(np_hist > benign_queries.shape[0])[0].shape[0]\n",
        "\n",
        "    round_detections.append(suspicious_detection/num_samples)\n",
        "\n",
        "    wrong_pred = np.where(np.argmax(model.predict(suspicious_queries), axis = -1) != suspicious_labels)[0]\n",
        "\n",
        "    x, y = perf_measure(suspicious_labels, np.argmax(model.predict(suspicious_queries), axis = -1))\n",
        "\n",
        "    missclass.append(wrong_pred.shape[0]/num_samples)\n",
        "    \n",
        "  return np.array(round_detections).mean(), np.array(missclass).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run report_attack_results function for a set of K values. \n",
        "# You can specify the value for number of generated samples (num_samples) and how many iteration to run the attack for each K (round_num)\n",
        "print(f'======== Encoder with {distance_type} distance metric, {100*portion_swap_pct}% swapping, {100*noise_pct}% noise and m^2 = {m_squared} ==========')\n",
        "num_samples = 1000\n",
        "round_num = 10\n",
        "\n",
        "detection_value, misclass_value = None, None\n",
        "\n",
        "for k in range(5, 51, 3):\n",
        "  print(f'----------- {k} ----------')\n",
        "  detection_value, misclass_value = report_attack_results(k, round_num, num_samples)\n",
        "  print('detection rate', detection_value, 'misclassification rate', misclass_value)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ENC_Notebook.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "489bbc54df30fb61df7836effe9596e0363dbc7d8e4a38bd4f4513df64efc24f"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('SQA': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
